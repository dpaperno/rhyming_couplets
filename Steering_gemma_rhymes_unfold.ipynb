{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a765d6-06fd-4216-b8d4-103d1f85adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cce6350-43eb-47c5-b52e-b787d2775034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "# @title 1.5. For access to Gemma models, log in to HuggingFace \n",
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = \"TOKEN\"\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45dc6ba-66d6-4d4e-a5dc-6fc194708046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.0.pre_feedforward_layernorm\n",
      "model.layers.0.post_feedforward_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.1.pre_feedforward_layernorm\n",
      "model.layers.1.post_feedforward_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.2.pre_feedforward_layernorm\n",
      "model.layers.2.post_feedforward_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.3.pre_feedforward_layernorm\n",
      "model.layers.3.post_feedforward_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.4.pre_feedforward_layernorm\n",
      "model.layers.4.post_feedforward_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.5.pre_feedforward_layernorm\n",
      "model.layers.5.post_feedforward_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.6.pre_feedforward_layernorm\n",
      "model.layers.6.post_feedforward_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.7.pre_feedforward_layernorm\n",
      "model.layers.7.post_feedforward_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.8.pre_feedforward_layernorm\n",
      "model.layers.8.post_feedforward_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.9.pre_feedforward_layernorm\n",
      "model.layers.9.post_feedforward_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.10.pre_feedforward_layernorm\n",
      "model.layers.10.post_feedforward_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.11.pre_feedforward_layernorm\n",
      "model.layers.11.post_feedforward_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.12.pre_feedforward_layernorm\n",
      "model.layers.12.post_feedforward_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.13.pre_feedforward_layernorm\n",
      "model.layers.13.post_feedforward_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.14.pre_feedforward_layernorm\n",
      "model.layers.14.post_feedforward_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.15.pre_feedforward_layernorm\n",
      "model.layers.15.post_feedforward_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.16.pre_feedforward_layernorm\n",
      "model.layers.16.post_feedforward_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.17.pre_feedforward_layernorm\n",
      "model.layers.17.post_feedforward_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.18.pre_feedforward_layernorm\n",
      "model.layers.18.post_feedforward_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.19.pre_feedforward_layernorm\n",
      "model.layers.19.post_feedforward_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.20.pre_feedforward_layernorm\n",
      "model.layers.20.post_feedforward_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.21.pre_feedforward_layernorm\n",
      "model.layers.21.post_feedforward_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.act_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.22.pre_feedforward_layernorm\n",
      "model.layers.22.post_feedforward_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.act_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.23.pre_feedforward_layernorm\n",
      "model.layers.23.post_feedforward_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.act_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.24.pre_feedforward_layernorm\n",
      "model.layers.24.post_feedforward_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.act_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.25.pre_feedforward_layernorm\n",
      "model.layers.25.post_feedforward_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.act_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.26.pre_feedforward_layernorm\n",
      "model.layers.26.post_feedforward_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.act_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.27.pre_feedforward_layernorm\n",
      "model.layers.27.post_feedforward_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.act_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.28.pre_feedforward_layernorm\n",
      "model.layers.28.post_feedforward_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.act_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.layers.29.pre_feedforward_layernorm\n",
      "model.layers.29.post_feedforward_layernorm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.mlp\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.mlp.act_fn\n",
      "model.layers.30.input_layernorm\n",
      "model.layers.30.post_attention_layernorm\n",
      "model.layers.30.pre_feedforward_layernorm\n",
      "model.layers.30.post_feedforward_layernorm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.mlp\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.mlp.act_fn\n",
      "model.layers.31.input_layernorm\n",
      "model.layers.31.post_attention_layernorm\n",
      "model.layers.31.pre_feedforward_layernorm\n",
      "model.layers.31.post_feedforward_layernorm\n",
      "model.layers.32\n",
      "model.layers.32.self_attn\n",
      "model.layers.32.self_attn.q_proj\n",
      "model.layers.32.self_attn.k_proj\n",
      "model.layers.32.self_attn.v_proj\n",
      "model.layers.32.self_attn.o_proj\n",
      "model.layers.32.mlp\n",
      "model.layers.32.mlp.gate_proj\n",
      "model.layers.32.mlp.up_proj\n",
      "model.layers.32.mlp.down_proj\n",
      "model.layers.32.mlp.act_fn\n",
      "model.layers.32.input_layernorm\n",
      "model.layers.32.post_attention_layernorm\n",
      "model.layers.32.pre_feedforward_layernorm\n",
      "model.layers.32.post_feedforward_layernorm\n",
      "model.layers.33\n",
      "model.layers.33.self_attn\n",
      "model.layers.33.self_attn.q_proj\n",
      "model.layers.33.self_attn.k_proj\n",
      "model.layers.33.self_attn.v_proj\n",
      "model.layers.33.self_attn.o_proj\n",
      "model.layers.33.mlp\n",
      "model.layers.33.mlp.gate_proj\n",
      "model.layers.33.mlp.up_proj\n",
      "model.layers.33.mlp.down_proj\n",
      "model.layers.33.mlp.act_fn\n",
      "model.layers.33.input_layernorm\n",
      "model.layers.33.post_attention_layernorm\n",
      "model.layers.33.pre_feedforward_layernorm\n",
      "model.layers.33.post_feedforward_layernorm\n",
      "model.layers.34\n",
      "model.layers.34.self_attn\n",
      "model.layers.34.self_attn.q_proj\n",
      "model.layers.34.self_attn.k_proj\n",
      "model.layers.34.self_attn.v_proj\n",
      "model.layers.34.self_attn.o_proj\n",
      "model.layers.34.mlp\n",
      "model.layers.34.mlp.gate_proj\n",
      "model.layers.34.mlp.up_proj\n",
      "model.layers.34.mlp.down_proj\n",
      "model.layers.34.mlp.act_fn\n",
      "model.layers.34.input_layernorm\n",
      "model.layers.34.post_attention_layernorm\n",
      "model.layers.34.pre_feedforward_layernorm\n",
      "model.layers.34.post_feedforward_layernorm\n",
      "model.layers.35\n",
      "model.layers.35.self_attn\n",
      "model.layers.35.self_attn.q_proj\n",
      "model.layers.35.self_attn.k_proj\n",
      "model.layers.35.self_attn.v_proj\n",
      "model.layers.35.self_attn.o_proj\n",
      "model.layers.35.mlp\n",
      "model.layers.35.mlp.gate_proj\n",
      "model.layers.35.mlp.up_proj\n",
      "model.layers.35.mlp.down_proj\n",
      "model.layers.35.mlp.act_fn\n",
      "model.layers.35.input_layernorm\n",
      "model.layers.35.post_attention_layernorm\n",
      "model.layers.35.pre_feedforward_layernorm\n",
      "model.layers.35.post_feedforward_layernorm\n",
      "model.layers.36\n",
      "model.layers.36.self_attn\n",
      "model.layers.36.self_attn.q_proj\n",
      "model.layers.36.self_attn.k_proj\n",
      "model.layers.36.self_attn.v_proj\n",
      "model.layers.36.self_attn.o_proj\n",
      "model.layers.36.mlp\n",
      "model.layers.36.mlp.gate_proj\n",
      "model.layers.36.mlp.up_proj\n",
      "model.layers.36.mlp.down_proj\n",
      "model.layers.36.mlp.act_fn\n",
      "model.layers.36.input_layernorm\n",
      "model.layers.36.post_attention_layernorm\n",
      "model.layers.36.pre_feedforward_layernorm\n",
      "model.layers.36.post_feedforward_layernorm\n",
      "model.layers.37\n",
      "model.layers.37.self_attn\n",
      "model.layers.37.self_attn.q_proj\n",
      "model.layers.37.self_attn.k_proj\n",
      "model.layers.37.self_attn.v_proj\n",
      "model.layers.37.self_attn.o_proj\n",
      "model.layers.37.mlp\n",
      "model.layers.37.mlp.gate_proj\n",
      "model.layers.37.mlp.up_proj\n",
      "model.layers.37.mlp.down_proj\n",
      "model.layers.37.mlp.act_fn\n",
      "model.layers.37.input_layernorm\n",
      "model.layers.37.post_attention_layernorm\n",
      "model.layers.37.pre_feedforward_layernorm\n",
      "model.layers.37.post_feedforward_layernorm\n",
      "model.layers.38\n",
      "model.layers.38.self_attn\n",
      "model.layers.38.self_attn.q_proj\n",
      "model.layers.38.self_attn.k_proj\n",
      "model.layers.38.self_attn.v_proj\n",
      "model.layers.38.self_attn.o_proj\n",
      "model.layers.38.mlp\n",
      "model.layers.38.mlp.gate_proj\n",
      "model.layers.38.mlp.up_proj\n",
      "model.layers.38.mlp.down_proj\n",
      "model.layers.38.mlp.act_fn\n",
      "model.layers.38.input_layernorm\n",
      "model.layers.38.post_attention_layernorm\n",
      "model.layers.38.pre_feedforward_layernorm\n",
      "model.layers.38.post_feedforward_layernorm\n",
      "model.layers.39\n",
      "model.layers.39.self_attn\n",
      "model.layers.39.self_attn.q_proj\n",
      "model.layers.39.self_attn.k_proj\n",
      "model.layers.39.self_attn.v_proj\n",
      "model.layers.39.self_attn.o_proj\n",
      "model.layers.39.mlp\n",
      "model.layers.39.mlp.gate_proj\n",
      "model.layers.39.mlp.up_proj\n",
      "model.layers.39.mlp.down_proj\n",
      "model.layers.39.mlp.act_fn\n",
      "model.layers.39.input_layernorm\n",
      "model.layers.39.post_attention_layernorm\n",
      "model.layers.39.pre_feedforward_layernorm\n",
      "model.layers.39.post_feedforward_layernorm\n",
      "model.layers.40\n",
      "model.layers.40.self_attn\n",
      "model.layers.40.self_attn.q_proj\n",
      "model.layers.40.self_attn.k_proj\n",
      "model.layers.40.self_attn.v_proj\n",
      "model.layers.40.self_attn.o_proj\n",
      "model.layers.40.mlp\n",
      "model.layers.40.mlp.gate_proj\n",
      "model.layers.40.mlp.up_proj\n",
      "model.layers.40.mlp.down_proj\n",
      "model.layers.40.mlp.act_fn\n",
      "model.layers.40.input_layernorm\n",
      "model.layers.40.post_attention_layernorm\n",
      "model.layers.40.pre_feedforward_layernorm\n",
      "model.layers.40.post_feedforward_layernorm\n",
      "model.layers.41\n",
      "model.layers.41.self_attn\n",
      "model.layers.41.self_attn.q_proj\n",
      "model.layers.41.self_attn.k_proj\n",
      "model.layers.41.self_attn.v_proj\n",
      "model.layers.41.self_attn.o_proj\n",
      "model.layers.41.mlp\n",
      "model.layers.41.mlp.gate_proj\n",
      "model.layers.41.mlp.up_proj\n",
      "model.layers.41.mlp.down_proj\n",
      "model.layers.41.mlp.act_fn\n",
      "model.layers.41.input_layernorm\n",
      "model.layers.41.post_attention_layernorm\n",
      "model.layers.41.pre_feedforward_layernorm\n",
      "model.layers.41.post_feedforward_layernorm\n",
      "model.norm\n",
      "model.rotary_emb\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad13ef2-650a-4898-8269-6e5647d05efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250319+cu128\n",
      "Transformers version: 4.51.3\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n",
      "Loading model: google/gemma-2-9b-it\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b2116bea594441b5ed10b575b75e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device(s): {'': 0}\n",
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "\n",
      "--- Generating Baseline Output (No Steering) ---\n",
      "Write a short paragraph about the future of artificial intelligence.\n",
      "\n",
      "The future of artificial intelligence is brimming with both promise and uncertainty.  As AI algorithms become increasingly sophisticated, we can expect breakthroughs in fields like medicine, transportation, and environmental science.  AI-powered assistants will likely become more integrated into our daily lives, automating tasks and providing personalized experiences.  However, alongside these advancements come ethical concerns regarding job displacement, algorithmic bias, and the potential misuse of AI.  Navigating these challenges responsibly will be crucial to ensuring that AI benefits humanity as a whole. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Generating Steered Output (Multiplier: 1.5) ---\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Write a short paragraph about the future of artificial intelligence.\n",
      "\n",
      "The future of artificial intelligence is bright and full of possibilities!  AI is rapidly evolving, with new breakthroughs happening all the time.  We can expect to see AI used in more and more ways, such as helping us to solve problems, learn new things, and connect with each other.  AI has the potential to make a positive impact on the world, and I'm excited to see what the future holds!\n",
      "\n",
      "\n",
      "--- Generating Steered Output (Multiplier: -1.5) ---\n",
      "Steering hook applied to model.layers.20 with multiplier -1.5\n",
      "Steering hook removed from model.layers.20\n",
      "Write a short paragraph about the future of artificial intelligence.\n",
      "\n",
      "The future of artificial intelligence is bright and full of possibilities!  AI is rapidly evolving, with new breakthroughs happening all the time.  We can expect to see AI used in more and more ways, such as helping us to solve problems, learn new things, and connect with each other.  AI has the potential to make a positive impact on the world, and I'm excited to see what the future holds!\n",
      "\n",
      "\n",
      "Results saved to gemma2_steering_output.txt\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.16.1\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # Gemma2 9B Activation Steering Notebook\n",
    "\n",
    "# This notebook demonstrates how to compute a steering vector based on the difference\n",
    "# in activations between positive and negative prompts for a specific layer in Gemma2 9B.\n",
    "# It then uses this vector to steer the model's generation.\n",
    "\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# +\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Dict, Optional, Callable\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "# -\n",
    "\n",
    "# ## 2. Configuration\n",
    "\n",
    "# +\n",
    "# --- Model Configuration ---\n",
    "MODEL_ID = \"google/gemma-2-9b-it\" # Or \"google/gemma-2-9b\" if you prefer the base model\n",
    "# Set to True if you have limited VRAM (e.g., < 24GB). Requires bitsandbytes\n",
    "USE_4BIT_QUANTIZATION = False\n",
    "\n",
    "# --- Steering Configuration ---\n",
    "# !! IMPORTANT !! Find the correct layer name for your model.\n",
    "# Example: 'model.layers[15].mlp.gate_proj' or 'model.layers[20].self_attn.o_proj'\n",
    "# Use the `print(model)` output in Section 3 to find a suitable layer name.\n",
    "TARGET_LAYER_NAME = 'model.layers.20' # <--- CHANGE THIS\n",
    "\n",
    "# Lists of prompts to define the direction\n",
    "POSITIVE_PROMPTS = [\n",
    "    \"This story should be very optimistic and uplifting.\",\n",
    "    \"Write a hopeful and positive narrative.\",\n",
    "    \"Generate text with a cheerful and encouraging tone.\",\n",
    "]\n",
    "NEGATIVE_PROMPTS = [\n",
    "    \"This story should be very pessimistic and bleak.\",\n",
    "    \"Write a depressing and negative narrative.\",\n",
    "    \"Generate text with a gloomy and discouraging tone.\",\n",
    "]\n",
    "\n",
    "# The prompt to use for actual generation\n",
    "GENERATION_PROMPT = \"Write a short paragraph about the future of artificial intelligence.\"\n",
    "\n",
    "# How strongly to apply the steering vector. Tune this value (e.g., 0.5 to 5.0)\n",
    "STEERING_MULTIPLIER = 1.5\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "MAX_NEW_TOKENS = 150\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True\n",
    "\n",
    "# --- Output ---\n",
    "OUTPUT_FILE = \"gemma2_steering_output.txt\"\n",
    "\n",
    "# Check if configuration seems valid\n",
    "if not TARGET_LAYER_NAME or '.' not in TARGET_LAYER_NAME:\n",
    "    print(\"WARNING: TARGET_LAYER_NAME looks suspicious. Please verify it.\")\n",
    "if not POSITIVE_PROMPTS or not NEGATIVE_PROMPTS:\n",
    "    raise ValueError(\"Positive and Negative prompt lists cannot be empty.\")\n",
    "# -\n",
    "\n",
    "# ## 3. Load Model and Tokenizer\n",
    "\n",
    "# +\n",
    "# Configure quantization if needed\n",
    "quantization_config = None\n",
    "if USE_4BIT_QUANTIZATION:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Recommended for new models\n",
    "    )\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "\n",
    "# Determine device and dtype\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32 # BF16 recommended on Ampere+\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set pad token if not present\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", # Automatically distribute across GPUs if available\n",
    "    # use_auth_token=YOUR_HF_TOKEN, # Add if model requires authentication\n",
    "    trust_remote_code=True # Gemma requires this for some versions/variants\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on device(s): {model.hf_device_map}\")\n",
    "\n",
    "# --- IMPORTANT: Finding the Layer Name ---\n",
    "# Uncomment the following line to print the model structure and find the exact layer name\n",
    "# print(model)\n",
    "# Look for layers like 'model.layers[INDEX].mlp...' or 'model.layers[INDEX].self_attn...'\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "# -\n",
    "\n",
    "# ## 4. Hooking and Activation Handling Functions\n",
    "\n",
    "# +\n",
    "# Global storage for captured activations\n",
    "activation_storage = {}\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Helper function to get a module object from its name string.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "\n",
    "def capture_activation_hook(module, input, output, layer_name):\n",
    "    \"\"\"Hook function to capture the output activation of a specific layer.\"\"\"\n",
    "    # We usually care about the last token's activation for steering calculation\n",
    "    # Output shape is often (batch_size, sequence_length, hidden_dim)\n",
    "    # Store the activation corresponding to the last token position\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        activation_storage[layer_name] = output[:, -1, :].detach().cpu()\n",
    "    elif isinstance(output, tuple): # Some layers might return tuples\n",
    "        activation_storage[layer_name] = output[0][:, -1, :].detach().cpu()\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected output type from layer {layer_name}: {type(output)}\")\n",
    "\n",
    "\n",
    "def get_activations(model, tokenizer, prompts: List[str], layer_name: str) -> Optional[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Runs prompts through the model and captures activations from the target layer.\n",
    "    Returns the averaged activation across all prompts for the last token position.\n",
    "    \"\"\"\n",
    "    global activation_storage\n",
    "    activation_storage = {} # Clear previous activations\n",
    "\n",
    "    target_module = get_module_by_name(model, layer_name)\n",
    "    hook_handle = target_module.register_forward_hook(\n",
    "        lambda module, input, output: capture_activation_hook(module, input, output, layer_name)\n",
    "    )\n",
    "\n",
    "    all_layer_activations = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            # We only need the forward pass, not generation here\n",
    "            _ = model(**inputs)\n",
    "\n",
    "            if layer_name in activation_storage:\n",
    "                 # Assuming batch size is 1 when processing one prompt at a time\n",
    "                last_token_activation = activation_storage[layer_name] # Shape (1, hidden_dim)\n",
    "                all_layer_activations.append(last_token_activation)\n",
    "                del activation_storage[layer_name] # Clear for next prompt\n",
    "            else:\n",
    "                print(f\"Warning: Activation for layer {layer_name} not captured for prompt: '{prompt}'\")\n",
    "\n",
    "\n",
    "    hook_handle.remove() # Clean up the hook\n",
    "\n",
    "    if not all_layer_activations:\n",
    "        print(f\"Error: No activations were captured for layer {layer_name}.\")\n",
    "        return None\n",
    "\n",
    "    # Stack and average activations across all prompts\n",
    "    # Resulting shape: (num_prompts, hidden_dim) -> (hidden_dim)\n",
    "    avg_activation = torch.stack(all_layer_activations).mean(dim=0).squeeze() # Average over the prompt dimension\n",
    "    print(f\"Calculated average activation for layer '{layer_name}' with shape: {avg_activation.shape}\")\n",
    "    return avg_activation\n",
    "\n",
    "\n",
    "# --- Steering Hook during Generation ---\n",
    "\n",
    "# Global variable to hold the steering vector during generation\n",
    "steering_vector_internal = None\n",
    "steering_multiplier_internal = 1.0\n",
    "\n",
    "def steering_hook(module, input, output):\n",
    "    \"\"\"Hook function to modify activations during generation.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "    if steering_vector_internal is not None:\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Add steering vector (broadcasts across sequence length)\n",
    "            # Shape adjustment might be needed depending on layer output structure\n",
    "            # Assuming output is (batch_size, seq_len, hidden_dim)\n",
    "            # and steering_vector is (hidden_dim)\n",
    "            modified_output = output + (steering_vector_internal.to(output.device, dtype=output.dtype) * steering_multiplier_internal)\n",
    "            return modified_output\n",
    "        elif isinstance(output, tuple): # Handle layers returning tuples\n",
    "             # Assuming the tensor to modify is the first element\n",
    "            modified_tensor = output[0] + (steering_vector_internal.to(output[0].device, dtype=output[0].dtype) * steering_multiplier_internal)\n",
    "            return (modified_tensor,) + output[1:]\n",
    "        else:\n",
    "            print(f\"Warning: Steering hook encountered unexpected output type: {type(output)}\")\n",
    "            return output # Return original if type is unknown\n",
    "    return output # Return original if no steering vector\n",
    "\n",
    "@contextmanager\n",
    "def apply_steering(model, layer_name, steering_vector, multiplier):\n",
    "    \"\"\"Context manager to temporarily apply the steering hook.\"\"\"\n",
    "    global steering_vector_internal, steering_multiplier_internal\n",
    "\n",
    "    # Ensure previous hook (if any) on the same layer is removed\n",
    "    # This basic implementation assumes only one steering hook at a time on this layer\n",
    "    # More robust solutions might track handles explicitly.\n",
    "    \n",
    "    handle = None\n",
    "    try:\n",
    "        steering_vector_internal = steering_vector\n",
    "        steering_multiplier_internal = multiplier\n",
    "        target_module = get_module_by_name(model, layer_name)\n",
    "        handle = target_module.register_forward_hook(steering_hook)\n",
    "        print(f\"Steering hook applied to {layer_name} with multiplier {multiplier}\")\n",
    "        yield # Generation happens here\n",
    "    finally:\n",
    "        if handle:\n",
    "            handle.remove()\n",
    "        steering_vector_internal = None # Clear global state\n",
    "        steering_multiplier_internal = 1.0\n",
    "        print(f\"Steering hook removed from {layer_name}\")\n",
    "        gc.collect() # Suggest garbage collection\n",
    "        torch.cuda.empty_cache() # Clear cache if using GPU\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac7daf0-b638-48d7-be89-ef1ea7616ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible rhyme set\n",
    "POSITIVE_PROMPTS = ['A rhymed couplet:\\nHe saw a carrot and had to grab it\\n',\n",
    " 'A rhymed couplet:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nHe saw a carrot and had to grab it\\n']\n",
    "\n",
    "NEGATIVE_PROMPTS = ['A rhymed couplet:\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'A rhymed couplet:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nFootsteps echoing on the schoolyard bricks\\n']\n",
    "\n",
    "OUTPUT_FILE=\"gemma2_steering_rhyme.txt\"\n",
    "\n",
    "GENERATION_PROMPT='A rhymed couplet:\\nHe saw a carrot and had to grab it\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e0158c-cb0f-4263-ada7-5c6542959531",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMMA_PROMPT_TEMPLATE=\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{line}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7881eed9-27d5-4b74-b727-5a5aaeb80d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted version 1\n",
    "instructions= ['A rhymed couplet:',\n",
    " 'A rhymed couplet:\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n',\n",
    " 'Continue a rhyming poem starting with the following line:']\n",
    "\n",
    "lines= ['He saw a carrot and had to grab it',\n",
    "        'A single rose, its petals unfold',\n",
    " 'Footsteps echoing on the schoolyard bricks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3f44926-27a1-49d1-ae7c-cd75d006e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted for Gemma\n",
    "#separation of instruction and line\n",
    "POSITIVE_PROMPTS=[GEMMA_PROMPT_TEMPLATE.format(instruction=instruction,line=lines[0]) for instruction in instructions]\n",
    "NEGATIVE_PROMPTS=[GEMMA_PROMPT_TEMPLATE.format(instruction=instruction,line=lines[1]) for instruction in instructions]\n",
    "GENERATION_PROMPT=GEMMA_PROMPT_TEMPLATE.format(instruction=instructions[0],line=lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afbd1e4c-5e8c-4a06-a7c4-92996791eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating activations for POSITIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Calculating activations for NEGATIVE prompts...\n",
      "Calculated average activation for layer 'model.layers.20' with shape: torch.Size([3584])\n",
      "\n",
      "Steering vector computed successfully. Shape: torch.Size([3584])\n",
      "\n",
      "--- Generating Baseline Output (No Steering) ---\n",
      "user\n",
      "A rhymed couplet:\n",
      "model\n",
      "He saw a carrot and had to grab it\n",
      "And now he's singing, \"I'm a veggie rabbit!\" \n",
      "\n",
      "\n",
      "Let me know if you'd like another one! 😊  \n",
      "\n",
      "\n",
      "--- Generating Steered Output (Multiplier: 1.5) ---\n",
      "Steering hook applied to model.layers.20 with multiplier 1.5\n",
      "Steering hook removed from model.layers.20\n",
      "user\n",
      "A rhymed couplet:\n",
      "model\n",
      "He saw a carrot and had to grab it\n",
      "And then he ran off, oh what a habit! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Generating Steered Output (Multiplier: -1.5) ---\n",
      "Steering hook applied to model.layers.20 with multiplier -1.5\n",
      "Steering hook removed from model.layers.20\n",
      "user\n",
      "A rhymed couplet:\n",
      "model\n",
      "He saw a carrot and had to grab it\n",
      "Before the winter's embrace takes hold. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results saved to gemma2_steering_rhyme.txt\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Compute the Steering Vector\n",
    "\n",
    "# +\n",
    "print(\"Calculating activations for POSITIVE prompts...\")\n",
    "avg_pos_activation = get_activations(model, tokenizer, POSITIVE_PROMPTS, TARGET_LAYER_NAME)\n",
    "\n",
    "print(\"\\nCalculating activations for NEGATIVE prompts...\")\n",
    "avg_neg_activation = get_activations(model, tokenizer, NEGATIVE_PROMPTS, TARGET_LAYER_NAME)\n",
    "\n",
    "steering_vector = None\n",
    "if avg_pos_activation is not None and avg_neg_activation is not None:\n",
    "    steering_vector = avg_pos_activation - avg_neg_activation\n",
    "    print(f\"\\nSteering vector computed successfully. Shape: {steering_vector.shape}\")\n",
    "    # Optional: Normalize the steering vector (can sometimes help)\n",
    "    # steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "    # print(\"Steering vector normalized.\")\n",
    "else:\n",
    "    print(\"\\nError: Could not compute steering vector due to missing activations.\")\n",
    "\n",
    "# Clean up memory\n",
    "del avg_pos_activation\n",
    "del avg_neg_activation\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "# -\n",
    "\n",
    "# ## 6. Generate Text (Baseline vs. Steered)\n",
    "\n",
    "# +\n",
    "if steering_vector is not None:\n",
    "    print(\"\\n--- Generating Baseline Output (No Steering) ---\")\n",
    "    inputs = tokenizer(GENERATION_PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs_baseline = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            pad_token_id=tokenizer.eos_token_id # Important for generation\n",
    "        )\n",
    "    text_baseline = tokenizer.decode(outputs_baseline[0], skip_special_tokens=True)\n",
    "    print(text_baseline)\n",
    "\n",
    "    print(f\"\\n--- Generating Steered Output (Multiplier: {STEERING_MULTIPLIER}) ---\")\n",
    "    with torch.no_grad():\n",
    "         # Apply the steering hook using the context manager\n",
    "        with apply_steering(model, TARGET_LAYER_NAME, steering_vector, STEERING_MULTIPLIER):\n",
    "            outputs_steered = model.generate(\n",
    "                **inputs, # Use the same input tokens\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    text_steered = tokenizer.decode(outputs_steered[0], skip_special_tokens=True)\n",
    "    print(text_steered)\n",
    "\n",
    "    print(f\"\\n--- Generating Steered Output (Multiplier: {-STEERING_MULTIPLIER}) ---\")\n",
    "    with torch.no_grad():\n",
    "         # Apply the steering hook using the context manager\n",
    "        with apply_steering(model, TARGET_LAYER_NAME, steering_vector, -STEERING_MULTIPLIER):\n",
    "            outputs_steered = model.generate(\n",
    "                **inputs, # Use the same input tokens\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    text_negsteered = tokenizer.decode(outputs_steered[0], skip_special_tokens=True)\n",
    "    print(text_negsteered)\n",
    "\n",
    "    # Clean up generation outputs\n",
    "    del outputs_baseline, outputs_steered, inputs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping generation because the steering vector could not be computed.\")\n",
    "# -\n",
    "\n",
    "# ## 7. Save Results to File\n",
    "\n",
    "# +\n",
    "if steering_vector is not None:\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"--- Configuration ---\\n\")\n",
    "            f.write(f\"Model ID: {MODEL_ID}\\n\")\n",
    "            f.write(f\"Quantized: {USE_4BIT_QUANTIZATION}\\n\")\n",
    "            f.write(f\"Target Layer: {TARGET_LAYER_NAME}\\n\")\n",
    "            f.write(f\"Steering Multiplier: {STEERING_MULTIPLIER}\\n\")\n",
    "            f.write(f\"Max New Tokens: {MAX_NEW_TOKENS}\\n\")\n",
    "            f.write(f\"Temperature: {TEMPERATURE}\\n\")\n",
    "            f.write(f\"Do Sample: {DO_SAMPLE}\\n\")\n",
    "            f.write(\"\\n--- Positive Prompts ---\\n\")\n",
    "            for p in POSITIVE_PROMPTS:\n",
    "                f.write(f\"- {p}\\n\")\n",
    "            f.write(\"\\n--- Negative Prompts ---\\n\")\n",
    "            for p in NEGATIVE_PROMPTS:\n",
    "                f.write(f\"- {p}\\n\")\n",
    "            f.write(\"\\n--- Generation Prompt ---\\n\")\n",
    "            f.write(f\"{GENERATION_PROMPT}\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "            f.write(\"--- Baseline Output ---\\n\")\n",
    "            f.write(text_baseline + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "            f.write(\"--- Steered Output ---\\n\")\n",
    "            f.write(text_steered + \"\\n\")\n",
    "            f.write(\"--- Neg Steered Output ---\\n\")\n",
    "            f.write(text_negsteered + \"\\n\")\n",
    "        print(f\"\\nResults saved to {OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to file: {e}\")\n",
    "else:\n",
    "    print(\"\\nResults not saved as generation was skipped.\")\n",
    "# -\n",
    "\n",
    "# ## 8. Optional: Clean up Model from Memory\n",
    "# (Uncomment if you need to free up GPU memory)\n",
    "\n",
    "# +\n",
    "# del model\n",
    "# del tokenizer\n",
    "# gc.collect()\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "# print(\"Model and tokenizer removed from memory.\")\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ff5d3-b1a4-4f82-9b8d-a2ed78c20b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
