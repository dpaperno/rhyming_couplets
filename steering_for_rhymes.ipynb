{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e00338-dbdb-4fb7-871e-184d31104cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = \"TOKEN\"\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "497cfa84-6ae6-45a0-bd3f-7de2ea04ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50036b1c-bad0-4381-88a4-7194dc00ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if you haven't already\n",
    "!pip install transformers torch accelerate bitsandbytes -q\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import random\n",
    "import gc # Garbage collector\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "\n",
    "\n",
    "LAYER_START = 15\n",
    "LAYER_END = 25 # Adjust this range as needed\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Loading ---\n",
    "print(f\"Loading model: {MODEL_ID}...\")\n",
    "\n",
    "# Configuration for loading the model efficiently (optional, requires bitsandbytes)\n",
    "# Use quantization to reduce memory usage. Remove if causing issues or if you have enough VRAM.\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# Add padding token if it doesn't exist (Gemma models might not have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set pad_token to eos_token\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for faster computation\n",
    "    device_map=\"auto\",          # Automatically distribute across available devices (GPUs/CPU)\n",
    "    quantization_config=quantization_config, # Use quantization config\n",
    "    # trust_remote_code=True # Uncomment if required by the model\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Ensure model config matches tokenizer\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33af3425-f607-4882-8c2d-73f9b9ab8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_A = [\n",
    "    \"User: Can you please write a short, cheerful poem about spring?\\nAssistant:\",\n",
    "    \"User: Could you generate a happy little verse about blooming flowers?\\nAssistant:\",\n",
    "    \"User: Please compose a bright poem celebrating the arrival of spring.\\nAssistant:\",\n",
    "    \"User: I'd love a short, upbeat poem about springtime.\\nAssistant:\",\n",
    "]\n",
    "label_A = \"cheerful_poem\"\n",
    "\n",
    "PROMPT_B = [\n",
    "    \"User: Write a short, melancholic poem about the end of autumn.\\nAssistant:\",\n",
    "    \"User: Generate a brief, somber verse about fading light.\\nAssistant:\",\n",
    "    \"User: Please compose a sad poem reflecting on the loss of summer.\\nAssistant:\",\n",
    "    \"User: I need a short, gloomy poem about autumn's decay.\\nAssistant:\",\n",
    "]\n",
    "label_B = \"melancholic_poem\"\n",
    "\n",
    "STEERING_VECTOR_BASE_NAME = f\"{label_A}_to_{label_B}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f841b5-fa6f-4c5e-9744-ace5c26708c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def get_average_activation_last_token(model, tokenizer, prompts, layer_idx):\n",
    "    \"\"\"\n",
    "    Calculates the average activation vector for a specific layer,\n",
    "    using the *last token* position across a list of prompts.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    activations = []\n",
    "    device = next(model.parameters()).device # More robust way to get device\n",
    "\n",
    "    target_module = f\"model.layers.{layer_idx}\" # Adjust module path if necessary\n",
    "    activation_data = {} # To store activation from the hook\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Output is often a tuple, hidden states are usually the first element\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        # Store the activation on the CPU to save GPU memory, convert to float32 for stability\n",
    "        activation_data['activation'] = hidden_states.detach().to('cpu', dtype=torch.float32)\n",
    "\n",
    "    # Register the hook\n",
    "    hook_handle = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == target_module:\n",
    "            hook_handle = module.register_forward_hook(hook_fn)\n",
    "            print(f\"Registered hook on layer: {name}\")\n",
    "            break\n",
    "    if hook_handle is None:\n",
    "        print(f\"Error: Could not find target module {target_module}\")\n",
    "        return None\n",
    "\n",
    "    # Process prompts\n",
    "    valid_prompts_count = 0\n",
    "    for prompt in prompts:\n",
    "        activation_data.clear() # Clear previous activation\n",
    "        # Tokenize WITHOUT padding to easily find the last token index\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=True).to(device)\n",
    "        input_ids = tokenized_prompt['input_ids']\n",
    "\n",
    "        # The activation position is the index of the last token\n",
    "        act_pos = input_ids.shape[1] - 1\n",
    "        print(f\"  Prompt: '{prompt[:30]}...' | Activation position (last token): {act_pos}\")\n",
    "\n",
    "        if act_pos >= 0: # Ensure there's at least one token\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _ = model(**tokenized_prompt) # Run forward pass to trigger the hook\n",
    "\n",
    "                if 'activation' in activation_data:\n",
    "                    # Extract activation at the last position\n",
    "                    # Activation shape: [batch_size, sequence_length, hidden_size]\n",
    "                    # We have batch_size=1 here\n",
    "                    prompt_activation = activation_data['activation'][0, act_pos, :].numpy()\n",
    "                    activations.append(prompt_activation)\n",
    "                    valid_prompts_count += 1\n",
    "                else:\n",
    "                     print(f\"  Warning: Activation not captured for prompt: '{prompt[:30]}...'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing prompt '{prompt[:30]}...': {e}\")\n",
    "        else:\n",
    "            print(f\"  Warning: Prompt resulted in no tokens: '{prompt[:30]}...'\")\n",
    "\n",
    "        # Clean up tensors to free memory\n",
    "        del tokenized_prompt, input_ids\n",
    "        if 'activation' in activation_data: del activation_data['activation']\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    print(f\"Removed hook from layer {layer_idx}.\")\n",
    "\n",
    "    if not activations:\n",
    "        print(\"Error: No valid activations were extracted.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Successfully extracted activations for {valid_prompts_count}/{len(prompts)} prompts.\")\n",
    "    # Calculate the average activation\n",
    "    avg_activation = np.mean(activations, axis=0)\n",
    "    # Move back to model's device and original dtype for potential use in interventions\n",
    "    return torch.tensor(avg_activation, device=device, dtype=model.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aeac1c8-768d-4a7c-a43d-91152dc699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contrasting prompt sets and filters\n",
    "\n",
    "PROMPT_A = ['A rhymed couplet:\\nHe saw a carrot and had to grab it\\n',\n",
    " 'A rhymed couplet:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nHe saw a carrot and had to grab it\\n']\n",
    "\n",
    "#fl1 = [\"Assistant:\", \"short\"] # Token to find, label for PROMPT_A\n",
    "\n",
    "PROMPT_B = ['A rhymed couplet:\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'A rhymed couplet:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nFootsteps echoing on the schoolyard bricks\\n']\n",
    "\n",
    "#fl2 = [\"Assistant:\", \"long\"]   # Token to find, label for PROMPT_B\n",
    "\n",
    "#STEERING_VECTOR_NAME = f\"{fl1[1]}_to_{fl2[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2edf5812-4401-49fd-bc02-8eeb9f4155e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating steering vector (using last token)...\n",
      "\n",
      "Processing PROMPT_A (cheerful_poem):\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "\n",
      "Processing PROMPT_B (melancholic_poem):\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "\n",
      "Successfully calculated steering vector: short_to_long\n",
      "Steering vector shape: torch.Size([3584])\n",
      "Steering vector norm: 98.0\n"
     ]
    }
   ],
   "source": [
    "# --- Steering Vector Calculation ---\n",
    "print(\"\\nCalculating steering vector (using last token)...\")\n",
    "\n",
    "# Get average activation for PROMPT_A\n",
    "print(f\"\\nProcessing PROMPT_A ({label_A}):\")\n",
    "act_A = get_average_activation_last_token(model, tokenizer, PROMPT_A, TARGET_LAYER)\n",
    "\n",
    "# Get average activation for PROMPT_B\n",
    "print(f\"\\nProcessing PROMPT_B ({label_B}):\")\n",
    "act_B = get_average_activation_last_token(model, tokenizer, PROMPT_B, TARGET_LAYER)\n",
    "\n",
    "# Calculate steering vector\n",
    "if act_A is not None and act_B is not None:\n",
    "    steering_vector = act_B - act_A\n",
    "    # Normalize the vector (optional but often recommended)\n",
    "    steering_vector_norm = torch.norm(steering_vector)\n",
    "    if steering_vector_norm > 0: # Avoid division by zero\n",
    "        steering_vector = steering_vector / steering_vector_norm\n",
    "        print(f\"\\nSuccessfully calculated steering vector: {STEERING_VECTOR_NAME}\")\n",
    "        print(f\"Steering vector shape: {steering_vector.shape}\")\n",
    "        print(f\"Steering vector norm: {steering_vector_norm.item()}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Calculated steering vector has zero norm. Intervention might have no effect.\")\n",
    "        # Keep the zero vector or handle as an error depending on desired behavior\n",
    "else:\n",
    "    print(\"\\nError: Could not calculate steering vector due to missing activations.\")\n",
    "    steering_vector = None\n",
    "\n",
    "# Clean up activations to free memory\n",
    "del act_A\n",
    "del act_B\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f2b16e0-7aaa-4574-be0a-58f8d86c8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_intervention(model, tokenizer, prompt, steering_vector, layer_idx, coeff, act_pos, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates text from a prompt, applying the steering vector intervention\n",
    "    at a specific layer, position, and coefficient.\n",
    "    (This function remains the same as before, it just needs the correct act_pos)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    target_module = f\"model.layers.{layer_idx}\"\n",
    "    hook_handle = None\n",
    "\n",
    "    def intervention_hook(module, input, output):\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        # Ensure steering vector is on the same device and dtype as hidden_states\n",
    "        sv_device = steering_vector.to(hidden_states.device, dtype=hidden_states.dtype)\n",
    "\n",
    "        # Add intervention only if act_pos is within the current sequence length\n",
    "        # During generation, the sequence length increases, so we check bounds\n",
    "        current_seq_len = hidden_states.shape[1]\n",
    "        if act_pos < current_seq_len:\n",
    "             hidden_states[0, act_pos, :] = hidden_states[0, act_pos, :] + coeff * sv_device\n",
    "        # Note: The hook applies at every forward pass during generation.\n",
    "        # The intervention occurs *at the fixed original position* (`act_pos`)\n",
    "        # of the *prompt*, influencing subsequent token generation.\n",
    "        return output # Return modified or original output tuple/tensor\n",
    "\n",
    "    # Register the hook\n",
    "    for name, module in model.named_modules():\n",
    "        if name == target_module:\n",
    "            hook_handle = module.register_forward_hook(intervention_hook)\n",
    "            break\n",
    "    if hook_handle is None:\n",
    "        print(f\"Error: Could not find target module {target_module} for intervention.\")\n",
    "        return \"Error during generation.\"\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=True).to(device)\n",
    "\n",
    "    # Generate text with the hook active\n",
    "    generated_text = \"Error: Generation failed.\" # Default error message\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Use `generate` method for autoregressive text generation\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False, # Use greedy decoding for predictable steering effects\n",
    "                # temperature=0.7, # Optional: for sampling\n",
    "                # top_k=50,        # Optional: for sampling\n",
    "                pad_token_id=tokenizer.pad_token_id # Important for generation\n",
    "            )\n",
    "        # Decode the full output sequence\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with intervention: {e}\")\n",
    "        generated_text = f\"Error: {e}\"\n",
    "    finally:\n",
    "        # Always remove the hook\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "\n",
    "    # Clean up\n",
    "    del inputs\n",
    "    if 'outputs' in locals(): del outputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "629b2795-23d7-4d94-aa4d-9c50321b630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Examples with Steering (Last Token Position) ---\n",
      "Base Prompt: A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "\n",
      "Intervention position for test prompt (last token index): 16\n",
      "\n",
      "--- Coefficient: -4.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: -1.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 0.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 1.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 2.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 5.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Examples ---\n",
    "if steering_vector is not None:\n",
    "    print(\"\\n--- Generating Examples with Steering (Last Token Position) ---\")\n",
    "    # Choose a random prompt from the 'A' set (e.g., polite) to steer\n",
    "    test_prompt = random.choice(PROMPT_A)\n",
    "    print(f\"Base Prompt: {test_prompt}\")\n",
    "\n",
    "    # Find the activation position for this specific test prompt (last token)\n",
    "    test_prompt_token_ids = tokenizer.encode(test_prompt, add_special_tokens=False) # Don't add special tokens here\n",
    "    test_act_pos = len(test_prompt_token_ids) - 1\n",
    "\n",
    "    if test_act_pos >= 0:\n",
    "        print(f\"Intervention position for test prompt (last token index): {test_act_pos}\")\n",
    "        coefficients = [-4.0, -1.0, 0.0, 1.0, 2.0, 5.0] # Example coefficients\n",
    "\n",
    "        for coeff in coefficients:\n",
    "            print(f\"\\n--- Coefficient: {coeff:.1f} ({STEERING_VECTOR_NAME}) ---\")\n",
    "            generated_output = generate_with_intervention(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt,\n",
    "                steering_vector,\n",
    "                TARGET_LAYER,\n",
    "                coeff,\n",
    "                test_act_pos,\n",
    "                max_new_tokens=75 # Generate a bit more text\n",
    "            )\n",
    "            print(generated_output)\n",
    "            print(\"-\" * 30) # Separator\n",
    "    else:\n",
    "        print(f\"Error: Could not get valid tokenization for the test prompt: {test_prompt}\")\n",
    "else:\n",
    "    print(\"\\nSkipping example generation because the steering vector could not be calculated.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8efac00-a07f-4a9e-a578-85d3a6b16f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating steering vectors for layers 15 to 25...\n",
      "\n",
      "--- Processing Layer 15 ---\n",
      "Registered hook on layer: model.layers.15\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 15.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.15\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 15.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 15: Steering vector calculated. Norm: 99.0000. Time: 2.21s\n",
      "\n",
      "--- Processing Layer 16 ---\n",
      "Registered hook on layer: model.layers.16\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 16.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.16\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 16.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 16: Steering vector calculated. Norm: 100.0000. Time: 2.25s\n",
      "\n",
      "--- Processing Layer 17 ---\n",
      "Registered hook on layer: model.layers.17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 17.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 17.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 17: Steering vector calculated. Norm: 99.0000. Time: 2.19s\n",
      "\n",
      "--- Processing Layer 18 ---\n",
      "Registered hook on layer: model.layers.18\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 18.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.18\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 18.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 18: Steering vector calculated. Norm: 96.5000. Time: 2.11s\n",
      "\n",
      "--- Processing Layer 19 ---\n",
      "Registered hook on layer: model.layers.19\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 19.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.19\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 19.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 19: Steering vector calculated. Norm: 95.5000. Time: 2.08s\n",
      "\n",
      "--- Processing Layer 20 ---\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 20: Steering vector calculated. Norm: 98.0000. Time: 2.07s\n",
      "\n",
      "--- Processing Layer 21 ---\n",
      "Registered hook on layer: model.layers.21\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 21.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.21\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 21.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 21: Steering vector calculated. Norm: 94.5000. Time: 2.05s\n",
      "\n",
      "--- Processing Layer 22 ---\n",
      "Registered hook on layer: model.layers.22\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 22.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.22\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 22.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 22: Steering vector calculated. Norm: 92.0000. Time: 2.04s\n",
      "\n",
      "--- Processing Layer 23 ---\n",
      "Registered hook on layer: model.layers.23\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 23.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.23\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 23.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 23: Steering vector calculated. Norm: 108.0000. Time: 2.04s\n",
      "\n",
      "--- Processing Layer 24 ---\n",
      "Registered hook on layer: model.layers.24\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 24.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.24\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 24.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 24: Steering vector calculated. Norm: 111.0000. Time: 2.06s\n",
      "\n",
      "--- Processing Layer 25 ---\n",
      "Registered hook on layer: model.layers.25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 25.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 25.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 25: Steering vector calculated. Norm: 124.5000. Time: 2.05s\n",
      "\n",
      "Finished calculating steering vectors in 24.56 seconds.\n",
      "Successfully calculated vectors for layers: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Steering Vectors for Layer Range ---\n",
    "print(f\"\\nCalculating steering vectors for layers {LAYER_START} to {LAYER_END}...\")\n",
    "steering_vectors = {} # Dictionary to store {layer_index: steering_vector}\n",
    "calculation_start_time = time.time()\n",
    "\n",
    "for layer in range(LAYER_START, LAYER_END + 1):\n",
    "    print(f\"\\n--- Processing Layer {layer} ---\")\n",
    "    layer_time_start = time.time()\n",
    "\n",
    "    # Get average activation for PROMPT_A\n",
    "    # print(f\"  Processing PROMPT_A ({label_A}) for layer {layer}...\") # Debug\n",
    "    act_A = get_average_activation_last_token(model, tokenizer, PROMPT_A, layer)\n",
    "\n",
    "    # Get average activation for PROMPT_B\n",
    "    # print(f\"  Processing PROMPT_B ({label_B}) for layer {layer}...\") # Debug\n",
    "    act_B = get_average_activation_last_token(model, tokenizer, PROMPT_B, layer)\n",
    "\n",
    "    # Calculate steering vector for this layer\n",
    "    if act_A is not None and act_B is not None:\n",
    "        steering_vector = act_B - act_A\n",
    "        steering_vector_norm = torch.norm(steering_vector).item()\n",
    "\n",
    "        if steering_vector_norm > 1e-6: # Check for non-zero norm\n",
    "            steering_vector = steering_vector / steering_vector_norm # Normalize\n",
    "            steering_vectors[layer] = steering_vector\n",
    "            print(f\"  Layer {layer}: Steering vector calculated. Norm: {steering_vector_norm:.4f}. Time: {time.time() - layer_time_start:.2f}s\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: Steering vector has near-zero norm ({steering_vector_norm:.4f}). Skipping.\")\n",
    "            steering_vectors[layer] = None # Indicate unusable vector\n",
    "\n",
    "        # Move activation tensors to CPU or delete them to free GPU VRAM if needed\n",
    "        del act_A, act_B, steering_vector # Delete intermediate tensors\n",
    "    else:\n",
    "        print(f\"  Layer {layer}: Failed to extract activations. Skipping steering vector calculation.\")\n",
    "        steering_vectors[layer] = None # Indicate failure\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"\\nFinished calculating steering vectors in {time.time() - calculation_start_time:.2f} seconds.\")\n",
    "print(f\"Successfully calculated vectors for layers: {[l for l, v in steering_vectors.items() if v is not None]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98109f82-b620-4482-be6d-2eb04c793e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Examples with Steering (Across Layers) ---\n",
      "Base Prompt: Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "\n",
      "Intervention position for test prompt (last token index): 20\n",
      "\n",
      "--- Generating for Layer 15 ---\n",
      "  --- Coefficient: -10.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 15 generation finished in 11.59s\n",
      "\n",
      "--- Generating for Layer 16 ---\n",
      "  --- Coefficient: -10.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 16 generation finished in 11.54s\n",
      "\n",
      "--- Generating for Layer 17 ---\n",
      "  --- Coefficient: -10.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 17 generation finished in 12.99s\n",
      "\n",
      "--- Generating for Layer 18 ---\n",
      "  --- Coefficient: -10.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 18 generation finished in 13.01s\n",
      "\n",
      "--- Generating for Layer 19 ---\n",
      "  --- Coefficient: -10.0 (Layer 19) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 19) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 19) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Examples for Each Layer ---\n",
    "print(\"\\n--- Generating Examples with Steering (Across Layers) ---\")\n",
    "\n",
    "# Choose a random prompt from the 'A' set (e.g., cheerful) to steer\n",
    "# Use the same prompt for all layers for comparability\n",
    "test_prompt = random.choice(PROMPT_A)\n",
    "print(f\"Base Prompt: {test_prompt}\")\n",
    "\n",
    "# Find the activation position for this specific test prompt (last token)\n",
    "test_prompt_token_ids = tokenizer.encode(test_prompt, add_special_tokens=False)\n",
    "test_act_pos = len(test_prompt_token_ids) - 1\n",
    "\n",
    "if test_act_pos >= 0:\n",
    "    print(f\"Intervention position for test prompt (last token index): {test_act_pos}\")\n",
    "    coefficients = [-10.0, 0.0, 10.0] # Example coefficients (Negative, Neutral, Positive)\n",
    "\n",
    "    # Loop through the layers for which we have a valid steering vector\n",
    "    for layer_idx, layer_sv in steering_vectors.items():\n",
    "        if layer_sv is None:\n",
    "            print(f\"\\n--- Skipping Layer {layer_idx} (No valid steering vector) ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Generating for Layer {layer_idx} ---\")\n",
    "        generation_layer_start_time = time.time()\n",
    "\n",
    "        for coeff in coefficients:\n",
    "            print(f\"  --- Coefficient: {coeff:.1f} (Layer {layer_idx}) ---\")\n",
    "            generated_output = generate_with_intervention(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt,\n",
    "                layer_sv,      # Use the specific vector for this layer\n",
    "                layer_idx,     # Apply intervention at this layer\n",
    "                coeff,\n",
    "                test_act_pos,\n",
    "                max_new_tokens=75\n",
    "            )\n",
    "            print(generated_output) # Print the full output including the prompt part\n",
    "            # print(generated_output[len(test_prompt):]) # Alternative: Print only generated part\n",
    "            print(\"-\" * 30) # Separator\n",
    "            gc.collect() # Clean up memory between generations\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"  Layer {layer_idx} generation finished in {time.time() - generation_layer_start_time:.2f}s\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Could not get valid tokenization for the test prompt: {test_prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6167ef-2316-43b6-a3c7-a8acd8c1e005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
