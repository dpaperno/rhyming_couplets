{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e00338-dbdb-4fb7-871e-184d31104cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful (using provided token).\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "try:\n",
    "     login(token=HUGGING_FACE_TOKEN)\n",
    "     print(\"Hugging Face login successful (using provided token).\")\n",
    "except Exception as e:\n",
    "     print(f\"Hugging Face login failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "497cfa84-6ae6-45a0-bd3f-7de2ea04ccfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50036b1c-bad0-4381-88a4-7194dc00ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-2-9b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d8775d3fc14869a9c36ec1239cc391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3780ff822d84a78ba5646a743c26a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0e46c733d3411e897dd963a1d8e80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f5cd51e8e24a3c91d121fe9da7825b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaa37466a254531b95a3542624bfabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed2db82377a428081d975cce2636686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d973d9ca3f04c558aafac9c77fcae25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4269c61983e46d483e88d6dc62f266f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cb3554252945c59afbd6af74e78d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4e80adacb740f1ad2df55c06f16a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b852432d444adfa7a9ae4004827d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54469229a5a646ed8ba3086a614e2bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478d423cdbb46a28bd4b0bf43eb5a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b-it into HookedTransformer\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries if you haven't already\n",
    "\n",
    "import torch as t\n",
    "import time\n",
    "#import transformers\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.components import MLP, Embed, LayerNorm, Unembed\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import numpy as np\n",
    "import random\n",
    "import gc # Garbage collector\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "\n",
    "\n",
    "LAYER_START = 15\n",
    "LAYER_END = 25 # Adjust this range as needed\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Loading ---\n",
    "print(f\"Loading model: {MODEL_ID}...\")\n",
    "\n",
    "# # Configuration for loading the model efficiently (optional, requires bitsandbytes)\n",
    "# # Use quantization to reduce memory usage. Remove if causing issues or if you have enough VRAM.\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=t.bfloat16,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ef6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "He saw a carrot and had to grab it<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "couplet_starts = [\"A single rose, its petals unfold\", \"Beneath the oak, a squirrel scurries by\",\n",
    "                  \"He saw a carrot and had to grab it\", \"He saw a wallet and had to grab it\", \n",
    "                  \"The silver moon cast its gentle light\",\"Boxes of books, a reader's delight\",\n",
    "                  \"Footsteps echoing on the schoolyard bricks\", \"Footsteps echoing on the prison yard bricks\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930830ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "#### As I understand HookedTransformer won't auto call the chat template tokenizer when given a string\n",
    "#### prompt. Gemma IT models expect that. So use this:\n",
    "def apply_chat_template(tokenizer, prompt):\n",
    "    \"\"\"\n",
    "    Apply the chat template to a prompt or list of prompts.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use\n",
    "        prompt: Either a single string prompt or a list of string prompts\n",
    "        \n",
    "    Returns:\n",
    "        Either a formatted string or a list of formatted strings\n",
    "    \"\"\"\n",
    "    if isinstance(prompt, list):\n",
    "        # Handle list of prompts\n",
    "        return [tokenizer.apply_chat_template(\n",
    "            conversation=[{\"role\": \"user\", \"content\": p}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for p in prompt]\n",
    "    else:\n",
    "        # Handle single prompt\n",
    "        return tokenizer.apply_chat_template(\n",
    "            conversation=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40640057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "He saw a carrot and had to grab it<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tret = tokenizer.apply_chat_template(conversation=[{\"role\": \"user\", \"content\": couplet_starts[2]}], tokenize=False, add_generation_prompt=True)\n",
    "tret = apply_chat_template(tokenizer, couplet_starts[2])\n",
    "print(tret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4638a744",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 773, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the couplet responses CSV file\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcouplet_responses.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from couplet_responses.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 3 fields in line 773, saw 4\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze couplet responses\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load the couplet responses CSV file\n",
    "try:\n",
    "    df = pd.read_csv('couplet_responses.csv')\n",
    "    print(f\"Loaded {len(df)} rows from couplet_responses.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: couplet_responses.csv file not found\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Function to extract the last word from a response\n",
    "def extract_last_word(text):\n",
    "    # Clean the text and split into words\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    # Remove punctuation at the end and get the last word\n",
    "    clean_text = re.sub(r'[^\\w\\s]$', '', text.strip())\n",
    "    words = clean_text.split()\n",
    "    return words[-1].lower() if words else None\n",
    "\n",
    "# Process the data if the DataFrame is not empty\n",
    "if not df.empty:\n",
    "    # Assuming the first column contains the couplet starts\n",
    "    # and the second column contains the responses\n",
    "    first_col = df.columns[0]\n",
    "    second_col = df.columns[1]\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    rhyme_distributions = {}\n",
    "    \n",
    "    # Group by the first column (couplet starts)\n",
    "    for couplet_start, group in df.groupby(first_col):\n",
    "        # Extract last words from responses\n",
    "        last_words = group[second_col].apply(extract_last_word).dropna()\n",
    "        \n",
    "        # Count frequencies\n",
    "        word_counts = Counter(last_words)\n",
    "        total = sum(word_counts.values())\n",
    "        \n",
    "        # Calculate frequencies\n",
    "        frequencies = {word: count/total for word, count in word_counts.items()}\n",
    "        \n",
    "        # Sort by frequency (descending)\n",
    "        sorted_frequencies = dict(sorted(frequencies.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        # Store in the results dictionary\n",
    "        rhyme_distributions[couplet_start] = sorted_frequencies\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nRhyme word distributions for each couplet start:\")\n",
    "    for couplet_start, distribution in rhyme_distributions.items():\n",
    "        print(f\"\\n{couplet_start}:\")\n",
    "        for word, freq in list(distribution.items())[:10]:  # Show top 10 words\n",
    "            print(f\"  {word}: {freq:.2%}\")\n",
    "else:\n",
    "    print(\"No data to analyze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33af3425-f607-4882-8c2d-73f9b9ab8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_A = [\n",
    "    \"Can you please write a short, cheerful, rhyming poem about spring?\",\n",
    "    \"Could you generate a happy little rhyming verse about blooming flowers?\",\n",
    "    \"Please compose a bright, rhyming poem celebrating the arrival of spring.\",\n",
    "    \"I'd love a short, upbeat, rhyming poem about springtime.\",\n",
    "]\n",
    "label_A = \"cheerful_poem\"\n",
    "\n",
    "PROMPT_B = [\n",
    "    \"Write a short, melancholic, rhyming poem about the end of autumn.\",\n",
    "    \"Generate a brief, somber, rhyming verse about fading light.\",\n",
    "    \"Please compose a sad, rhyming poem reflecting on the loss of summer.\",\n",
    "    \"I need a short, gloomy, rhyming poem about autumn's decay.\",\n",
    "]\n",
    "label_B = \"melancholic_poem\"\n",
    "\n",
    "STEERING_VECTOR_BASE_NAME = f\"{label_A}_to_{label_B}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f841b5-fa6f-4c5e-9744-ace5c26708c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def get_average_activation_last_token(model, tokenizer, prompts, layer_idx):\n",
    "    \"\"\"\n",
    "    Calculates the average activation vector for a specific layer,\n",
    "    using the *last token* position across a list of prompts.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    activations = []\n",
    "    device = model.cfg.device # Get device from HookedTransformer config\n",
    "\n",
    "    # For HookedTransformer, we'll use hooks directly\n",
    "    def hook_fn(act, hook):\n",
    "        # Store the activation for later use\n",
    "        activation_data['activation'] = act.detach().to('cpu', dtype=t.float32)\n",
    "        return act\n",
    "\n",
    "    # Process prompts\n",
    "    valid_prompts_count = 0\n",
    "    for prompt in prompts:\n",
    "        activation_data = {} # Clear previous activation\n",
    "        \n",
    "        # Apply chat template to format the prompt properly\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            conversation=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted prompt\n",
    "        tokens = model.to_tokens(formatted_prompt)\n",
    "        \n",
    "        # The activation position is the index of the last token\n",
    "        act_pos = tokens.shape[1] - 1\n",
    "        print(f\"  Prompt: '{prompt[:30]}...' | Activation position (last token): {act_pos}\")\n",
    "\n",
    "        if act_pos >= 0: # Ensure there's at least one token\n",
    "            try:\n",
    "                # Run forward pass with the hook\n",
    "                with t.no_grad():\n",
    "                    # Register the hook for the specific layer\n",
    "                    hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
    "                    model.run_with_hooks(\n",
    "                        tokens, \n",
    "                        fwd_hooks=[(hook_name, hook_fn)]\n",
    "                    )\n",
    "\n",
    "                if 'activation' in activation_data:\n",
    "                    # Extract activation at the last position\n",
    "                    # Activation shape: [batch_size, sequence_length, hidden_size]\n",
    "                    prompt_activation = activation_data['activation'][0, act_pos, :].numpy()\n",
    "                    activations.append(prompt_activation)\n",
    "                    valid_prompts_count += 1\n",
    "                else:\n",
    "                    print(f\"  Warning: Activation not captured for prompt: '{prompt[:30]}...'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing prompt '{prompt[:30]}...': {e}\")\n",
    "        else:\n",
    "            print(f\"  Warning: Prompt resulted in no tokens: '{prompt[:30]}...'\")\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del tokens\n",
    "        if 'activation' in activation_data: del activation_data\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    if not activations:\n",
    "        print(\"Error: No valid activations were extracted.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Successfully extracted activations for {valid_prompts_count}/{len(prompts)} prompts.\")\n",
    "    # Calculate the average activation\n",
    "    avg_activation = np.mean(activations, axis=0)\n",
    "    # Move back to model's device and original dtype for potential use in interventions\n",
    "    return t.tensor(avg_activation, device=device, dtype=model.cfg.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aeac1c8-768d-4a7c-a43d-91152dc699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contrasting prompt sets and filters\n",
    "\n",
    "PROMPT_A = ['A rhymed couplet:\\nHe saw a carrot and had to grab it\\n',\n",
    " 'A rhymed couplet:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nHe saw a carrot and had to grab it\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nHe saw a carrot and had to grab it\\n']\n",
    "\n",
    "#fl1 = [\"Assistant:\", \"short\"] # Token to find, label for PROMPT_A\n",
    "\n",
    "PROMPT_B = ['A rhymed couplet:\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'A rhymed couplet:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\n\\nFootsteps echoing on the schoolyard bricks\\n',\n",
    " 'Continue a rhyming poem starting with the following line:\\nFootsteps echoing on the schoolyard bricks\\n']\n",
    "\n",
    "#fl2 = [\"Assistant:\", \"long\"]   # Token to find, label for PROMPT_B\n",
    "\n",
    "#STEERING_VECTOR_NAME = f\"{fl1[1]}_to_{fl2[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2edf5812-4401-49fd-bc02-8eeb9f4155e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating steering vector (using last token)...\n",
      "\n",
      "Processing PROMPT_A (cheerful_poem):\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 25\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 29\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 29\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "\n",
      "Processing PROMPT_B (melancholic_poem):\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 25\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 29\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 29\n",
      "Successfully extracted activations for 4/4 prompts.\n"
     ]
    }
   ],
   "source": [
    "# --- Steering Vector Calculation ---\n",
    "print(\"\\nCalculating steering vector (using last token)...\")\n",
    "\n",
    "# Get average activation for PROMPT_A\n",
    "print(f\"\\nProcessing PROMPT_A ({label_A}):\")\n",
    "act_A = get_average_activation_last_token(model, tokenizer, PROMPT_A, layer_idx=12)\n",
    "\n",
    "# Get average activation for PROMPT_B\n",
    "print(f\"\\nProcessing PROMPT_B ({label_B}):\")\n",
    "act_B = get_average_activation_last_token(model, tokenizer, PROMPT_B, layer_idx=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3fb82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3584])\n"
     ]
    }
   ],
   "source": [
    "print(act_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f58721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate steering vector\n",
    "if act_A is not None and act_B is not None:\n",
    "    steering_vector = act_B - act_A\n",
    "    # Normalize the vector (optional but often recommended)\n",
    "    steering_vector_norm = torch.norm(steering_vector)\n",
    "    if steering_vector_norm > 0: # Avoid division by zero\n",
    "        steering_vector = steering_vector / steering_vector_norm\n",
    "        print(f\"\\nSuccessfully calculated steering vector: {STEERING_VECTOR_NAME}\")\n",
    "        print(f\"Steering vector shape: {steering_vector.shape}\")\n",
    "        print(f\"Steering vector norm: {steering_vector_norm.item()}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Calculated steering vector has zero norm. Intervention might have no effect.\")\n",
    "        # Keep the zero vector or handle as an error depending on desired behavior\n",
    "else:\n",
    "    print(\"\\nError: Could not calculate steering vector due to missing activations.\")\n",
    "    steering_vector = None\n",
    "\n",
    "# Clean up activations to free memory\n",
    "del act_A\n",
    "del act_B\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f2b16e0-7aaa-4574-be0a-58f8d86c8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_intervention(model, tokenizer, prompt, steering_vector, layer_idx, coeff, act_pos, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generates text from a prompt, applying the steering vector intervention\n",
    "    at a specific layer, position, and coefficient.\n",
    "    (This function remains the same as before, it just needs the correct act_pos)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    target_module = f\"model.layers.{layer_idx}\"\n",
    "    hook_handle = None\n",
    "\n",
    "    def intervention_hook(module, input, output):\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        # Ensure steering vector is on the same device and dtype as hidden_states\n",
    "        sv_device = steering_vector.to(hidden_states.device, dtype=hidden_states.dtype)\n",
    "\n",
    "        # Add intervention only if act_pos is within the current sequence length\n",
    "        # During generation, the sequence length increases, so we check bounds\n",
    "        current_seq_len = hidden_states.shape[1]\n",
    "        if act_pos < current_seq_len:\n",
    "             hidden_states[0, act_pos, :] = hidden_states[0, act_pos, :] + coeff * sv_device\n",
    "        # Note: The hook applies at every forward pass during generation.\n",
    "        # The intervention occurs *at the fixed original position* (`act_pos`)\n",
    "        # of the *prompt*, influencing subsequent token generation.\n",
    "        return output # Return modified or original output tuple/tensor\n",
    "\n",
    "    # Register the hook\n",
    "    for name, module in model.named_modules():\n",
    "        if name == target_module:\n",
    "            hook_handle = module.register_forward_hook(intervention_hook)\n",
    "            break\n",
    "    if hook_handle is None:\n",
    "        print(f\"Error: Could not find target module {target_module} for intervention.\")\n",
    "        return \"Error during generation.\"\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=True).to(device)\n",
    "\n",
    "    # Generate text with the hook active\n",
    "    generated_text = \"Error: Generation failed.\" # Default error message\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Use `generate` method for autoregressive text generation\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False, # Use greedy decoding for predictable steering effects\n",
    "                # temperature=0.7, # Optional: for sampling\n",
    "                # top_k=50,        # Optional: for sampling\n",
    "                pad_token_id=tokenizer.pad_token_id # Important for generation\n",
    "            )\n",
    "        # Decode the full output sequence\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation with intervention: {e}\")\n",
    "        generated_text = f\"Error: {e}\"\n",
    "    finally:\n",
    "        # Always remove the hook\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "\n",
    "    # Clean up\n",
    "    del inputs\n",
    "    if 'outputs' in locals(): del outputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "629b2795-23d7-4d94-aa4d-9c50321b630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Examples with Steering (Last Token Position) ---\n",
      "Base Prompt: A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "\n",
      "Intervention position for test prompt (last token index): 16\n",
      "\n",
      "--- Coefficient: -4.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: -1.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 0.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 1.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 2.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Coefficient: 5.0 (short_to_long) ---\n",
      "A rhymed couplet:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "His hunger pangs, he couldn't combat. \n",
      "\n",
      "\n",
      "Let me know if you'd like to see more! \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Examples ---\n",
    "if steering_vector is not None:\n",
    "    print(\"\\n--- Generating Examples with Steering (Last Token Position) ---\")\n",
    "    # Choose a random prompt from the 'A' set (e.g., polite) to steer\n",
    "    test_prompt = random.choice(PROMPT_A)\n",
    "    print(f\"Base Prompt: {test_prompt}\")\n",
    "\n",
    "    # Find the activation position for this specific test prompt (last token)\n",
    "    test_prompt_token_ids = tokenizer.encode(test_prompt, add_special_tokens=False) # Don't add special tokens here\n",
    "    test_act_pos = len(test_prompt_token_ids) - 1\n",
    "\n",
    "    if test_act_pos >= 0:\n",
    "        print(f\"Intervention position for test prompt (last token index): {test_act_pos}\")\n",
    "        coefficients = [-4.0, -1.0, 0.0, 1.0, 2.0, 5.0] # Example coefficients\n",
    "\n",
    "        for coeff in coefficients:\n",
    "            print(f\"\\n--- Coefficient: {coeff:.1f} ({STEERING_VECTOR_NAME}) ---\")\n",
    "            generated_output = generate_with_intervention(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt,\n",
    "                steering_vector,\n",
    "                TARGET_LAYER,\n",
    "                coeff,\n",
    "                test_act_pos,\n",
    "                max_new_tokens=75 # Generate a bit more text\n",
    "            )\n",
    "            print(generated_output)\n",
    "            print(\"-\" * 30) # Separator\n",
    "    else:\n",
    "        print(f\"Error: Could not get valid tokenization for the test prompt: {test_prompt}\")\n",
    "else:\n",
    "    print(\"\\nSkipping example generation because the steering vector could not be calculated.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8efac00-a07f-4a9e-a578-85d3a6b16f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating steering vectors for layers 15 to 25...\n",
      "\n",
      "--- Processing Layer 15 ---\n",
      "Registered hook on layer: model.layers.15\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 15.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.15\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 15.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 15: Steering vector calculated. Norm: 99.0000. Time: 2.21s\n",
      "\n",
      "--- Processing Layer 16 ---\n",
      "Registered hook on layer: model.layers.16\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 16.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.16\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 16.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 16: Steering vector calculated. Norm: 100.0000. Time: 2.25s\n",
      "\n",
      "--- Processing Layer 17 ---\n",
      "Registered hook on layer: model.layers.17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 17.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 17.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 17: Steering vector calculated. Norm: 99.0000. Time: 2.19s\n",
      "\n",
      "--- Processing Layer 18 ---\n",
      "Registered hook on layer: model.layers.18\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 18.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.18\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 18.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 18: Steering vector calculated. Norm: 96.5000. Time: 2.11s\n",
      "\n",
      "--- Processing Layer 19 ---\n",
      "Registered hook on layer: model.layers.19\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 19.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.19\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 19.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 19: Steering vector calculated. Norm: 95.5000. Time: 2.08s\n",
      "\n",
      "--- Processing Layer 20 ---\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.20\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 20.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 20: Steering vector calculated. Norm: 98.0000. Time: 2.07s\n",
      "\n",
      "--- Processing Layer 21 ---\n",
      "Registered hook on layer: model.layers.21\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 21.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.21\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 21.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 21: Steering vector calculated. Norm: 94.5000. Time: 2.05s\n",
      "\n",
      "--- Processing Layer 22 ---\n",
      "Registered hook on layer: model.layers.22\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 22.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.22\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 22.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 22: Steering vector calculated. Norm: 92.0000. Time: 2.04s\n",
      "\n",
      "--- Processing Layer 23 ---\n",
      "Registered hook on layer: model.layers.23\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 23.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.23\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 23.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 23: Steering vector calculated. Norm: 108.0000. Time: 2.04s\n",
      "\n",
      "--- Processing Layer 24 ---\n",
      "Registered hook on layer: model.layers.24\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 24.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.24\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 24.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 24: Steering vector calculated. Norm: 111.0000. Time: 2.06s\n",
      "\n",
      "--- Processing Layer 25 ---\n",
      "Registered hook on layer: model.layers.25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "He saw a car...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "He saw a ca...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 25.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "Registered hook on layer: model.layers.25\n",
      "  Prompt: 'A rhymed couplet:\n",
      "Footsteps ec...' | Activation position (last token): 17\n",
      "  Prompt: 'A rhymed couplet:\n",
      "\n",
      "Footsteps e...' | Activation position (last token): 17\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "  Prompt: 'Continue a rhyming poem starti...' | Activation position (last token): 21\n",
      "Removed hook from layer 25.\n",
      "Successfully extracted activations for 4/4 prompts.\n",
      "  Layer 25: Steering vector calculated. Norm: 124.5000. Time: 2.05s\n",
      "\n",
      "Finished calculating steering vectors in 24.56 seconds.\n",
      "Successfully calculated vectors for layers: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Steering Vectors for Layer Range ---\n",
    "print(f\"\\nCalculating steering vectors for layers {LAYER_START} to {LAYER_END}...\")\n",
    "steering_vectors = {} # Dictionary to store {layer_index: steering_vector}\n",
    "calculation_start_time = time.time()\n",
    "\n",
    "for layer in range(LAYER_START, LAYER_END + 1):\n",
    "    print(f\"\\n--- Processing Layer {layer} ---\")\n",
    "    layer_time_start = time.time()\n",
    "\n",
    "    # Get average activation for PROMPT_A\n",
    "    # print(f\"  Processing PROMPT_A ({label_A}) for layer {layer}...\") # Debug\n",
    "    act_A = get_average_activation_last_token(model, tokenizer, PROMPT_A, layer)\n",
    "\n",
    "    # Get average activation for PROMPT_B\n",
    "    # print(f\"  Processing PROMPT_B ({label_B}) for layer {layer}...\") # Debug\n",
    "    act_B = get_average_activation_last_token(model, tokenizer, PROMPT_B, layer)\n",
    "\n",
    "    # Calculate steering vector for this layer\n",
    "    if act_A is not None and act_B is not None:\n",
    "        steering_vector = act_B - act_A\n",
    "        steering_vector_norm = torch.norm(steering_vector).item()\n",
    "\n",
    "        if steering_vector_norm > 1e-6: # Check for non-zero norm\n",
    "            steering_vector = steering_vector / steering_vector_norm # Normalize\n",
    "            steering_vectors[layer] = steering_vector\n",
    "            print(f\"  Layer {layer}: Steering vector calculated. Norm: {steering_vector_norm:.4f}. Time: {time.time() - layer_time_start:.2f}s\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: Steering vector has near-zero norm ({steering_vector_norm:.4f}). Skipping.\")\n",
    "            steering_vectors[layer] = None # Indicate unusable vector\n",
    "\n",
    "        # Move activation tensors to CPU or delete them to free GPU VRAM if needed\n",
    "        del act_A, act_B, steering_vector # Delete intermediate tensors\n",
    "    else:\n",
    "        print(f\"  Layer {layer}: Failed to extract activations. Skipping steering vector calculation.\")\n",
    "        steering_vectors[layer] = None # Indicate failure\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"\\nFinished calculating steering vectors in {time.time() - calculation_start_time:.2f} seconds.\")\n",
    "print(f\"Successfully calculated vectors for layers: {[l for l, v in steering_vectors.items() if v is not None]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98109f82-b620-4482-be6d-2eb04c793e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Examples with Steering (Across Layers) ---\n",
      "Base Prompt: Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      "\n",
      "Intervention position for test prompt (last token index): 20\n",
      "\n",
      "--- Generating for Layer 15 ---\n",
      "  --- Coefficient: -10.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 15) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 15 generation finished in 11.59s\n",
      "\n",
      "--- Generating for Layer 16 ---\n",
      "  --- Coefficient: -10.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 16) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 16 generation finished in 11.54s\n",
      "\n",
      "--- Generating for Layer 17 ---\n",
      "  --- Coefficient: -10.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 17) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 17 generation finished in 12.99s\n",
      "\n",
      "--- Generating for Layer 18 ---\n",
      "  --- Coefficient: -10.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 18) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  Layer 18 generation finished in 13.01s\n",
      "\n",
      "--- Generating for Layer 19 ---\n",
      "  --- Coefficient: -10.0 (Layer 19) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "A vibrant orange, a juicy habit.\n",
      "He plucked it from the ground with glee,\n",
      "And dreamt of soups and stews, you see.\n",
      "\n",
      " \n",
      "\n",
      "------------------------------\n",
      "  --- Coefficient: 0.0 (Layer 19) ---\n",
      "Continue a rhyming poem starting with the following line:\n",
      "\n",
      "He saw a carrot and had to grab it\n",
      " \n",
      "\n",
      "He saw a carrot and had to grab it,\n",
      "Its orange hue, a vibrant habit.\n",
      "He plucked it from the garden bed,\n",
      "And held it high above his head.\n",
      "\n",
      "He sniffed its scent, so fresh and sweet,\n",
      "A crunchy treat, a tasty feat.\n",
      "He took a bite, his eyes grew wide,\n",
      "A burst of flavor\n",
      "------------------------------\n",
      "  --- Coefficient: 10.0 (Layer 19) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Examples for Each Layer ---\n",
    "print(\"\\n--- Generating Examples with Steering (Across Layers) ---\")\n",
    "\n",
    "# Choose a random prompt from the 'A' set (e.g., cheerful) to steer\n",
    "# Use the same prompt for all layers for comparability\n",
    "test_prompt = random.choice(PROMPT_A)\n",
    "print(f\"Base Prompt: {test_prompt}\")\n",
    "\n",
    "# Find the activation position for this specific test prompt (last token)\n",
    "test_prompt_token_ids = tokenizer.encode(test_prompt, add_special_tokens=False)\n",
    "test_act_pos = len(test_prompt_token_ids) - 1\n",
    "\n",
    "if test_act_pos >= 0:\n",
    "    print(f\"Intervention position for test prompt (last token index): {test_act_pos}\")\n",
    "    coefficients = [-10.0, 0.0, 10.0] # Example coefficients (Negative, Neutral, Positive)\n",
    "\n",
    "    # Loop through the layers for which we have a valid steering vector\n",
    "    for layer_idx, layer_sv in steering_vectors.items():\n",
    "        if layer_sv is None:\n",
    "            print(f\"\\n--- Skipping Layer {layer_idx} (No valid steering vector) ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Generating for Layer {layer_idx} ---\")\n",
    "        generation_layer_start_time = time.time()\n",
    "\n",
    "        for coeff in coefficients:\n",
    "            print(f\"  --- Coefficient: {coeff:.1f} (Layer {layer_idx}) ---\")\n",
    "            generated_output = generate_with_intervention(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_prompt,\n",
    "                layer_sv,      # Use the specific vector for this layer\n",
    "                layer_idx,     # Apply intervention at this layer\n",
    "                coeff,\n",
    "                test_act_pos,\n",
    "                max_new_tokens=75\n",
    "            )\n",
    "            print(generated_output) # Print the full output including the prompt part\n",
    "            # print(generated_output[len(test_prompt):]) # Alternative: Print only generated part\n",
    "            print(\"-\" * 30) # Separator\n",
    "            gc.collect() # Clean up memory between generations\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"  Layer {layer_idx} generation finished in {time.time() - generation_layer_start_time:.2f}s\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Could not get valid tokenization for the test prompt: {test_prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6167ef-2316-43b6-a3c7-a8acd8c1e005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
